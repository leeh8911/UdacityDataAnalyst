{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "In this case study, you'll apply what you've learned on confidence intervals and hypothesis testing to help a company decide whether to launch two new features on their website. To do this, you'll analyze results from A/B testing, a valuable and widely practiced method in industry."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A/B Testing\n",
    "A/B tests are used to test changes on a web page by running an experiment where a **control group** sees the old version, while the **experiment group** sees the new version. A **metric** is then chosen to measure the level of engagement from users in each group. These results are then used to judge whether one version is more effective than the other. A/B testing is very much like hypothesis testing with the following hypotheses:\n",
    "\n",
    "* **Null Hypothesis**: The new version is no better, or even worse, than the old version\n",
    "* **Alternative Hypothesis**: The new version is better than the old version\n",
    "\n",
    "If we fail to reject the null hypothesis, the results would suggest keeping the old version. If we reject the null hypothesis, the results would suggest launching the change. These tests can be used for a wide variety of changes, from large feature additions to small adjustments in color, to see what change maximizes your metric the most.\n",
    "\n",
    "A/B testing also has its drawbacks. It can help you compare two options, but it can't tell you about an option you haven’t considered. It can also produce bias results when tested on existing users, due to factors like change aversion and novelty effect.\n",
    "\n",
    "* **Change Aversion**: Existing users may give an unfair advantage to the old version, simply because they are unhappy with change, even if it’s ultimately for the better.\n",
    "* **Novelty Effect**: Existing users may give an unfair advantage to the new version, because they’re excited or drawn to the change, even if it isn’t any better in the long run.\n",
    "\n",
    "You'll learn more about factors like these later.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Business Example\n",
    "In this case study, you’ll analyze A/B test results for Audacity. Here's the customer funnel for typical new users on their site:\n",
    "\n",
    "**View home page > Explore courses > View course overview page > Enroll in course > Complete course**\n",
    "\n",
    "Audacity loses users as they go down the stages of this funnel, with only a few making it to the end. To increase student engagement, Audacity is performing A/B tests to try out changes that will hopefully increase conversion rates from one stage to the next.\n",
    "\n",
    "We’ll analyze test results for two changes they have in mind, and then make a recommendation on whether they should launch each change."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments I\n",
    "The first change Audacity wants to try is on their homepage. They hope that this new, more engaging design will increase the number of users that explore their courses, that is, move on to the second stage of the funnel.\n",
    "\n",
    "The metric we will use is the click through rate for the Explore Courses button on the home page. **Click through rate (CTR)** is often defined as the the number of clicks divided by the number of views. Since Audacity uses cookies, we can identify unique users and make sure we don't count the same one multiple times. For this experiment, we'll define our click through rate as:\n",
    "\n",
    "**CTR: # clicks by unique users / # views by unique users**\n",
    "\n",
    "Now that we have our metric, let's set up our null and alternative hypotheses:\n",
    "\n",
    "$H_0: \\text{CTR}_{\\text{new}} \\leq \\text{CTR}_{\\text{old}}$\n",
    "\n",
    "$H_1: \\text{CTR}_{\\text{new}} > \\text{CTR}_{\\text{old}}$\n",
    "\n",
    "Our alternative hypothesis is what we want to prove to be true, in this case, that the new homepage design has a higher click through rate than the old homepage design. And the null hypothesis is what we assume to be true before analyzing data, which is that the new homepage design has a click through rate that is less than or equal to that of the old homepage design. As you’ve seen before, we can rearrange our hypotheses to look like this:\n",
    "\n",
    "$H_0: \\text{CTR}_{\\text{new}} -\\text{CTR}_{\\text{old}} \\leq 0$\n",
    "\n",
    "$H_1: \\text{CTR}_{\\text{new}} -\\text{CTR}_{\\text{old}} > 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metric - Click Through Rate\n",
    "\n",
    "Let's recap the steps we took to analyze the results of this A/B test.\n",
    "\n",
    "1. We computed the **observed difference** between the metric, click through rate, for the control and experiment group.\n",
    "1. We simulated the **sampling distribution** for the difference in proportions (or difference in click through rates).\n",
    "1. We used this sampling distribution to simulate the **distribution under the null** hypothesis, by creating a random normal distribution centered at 0 with the same spread and size.\n",
    "1. We computed the **p-value** by finding the proportion of values in the null distribution that were greater than our observed difference.\n",
    "1. We used this p-value to determine the **statistical significance** of our observed difference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment II\n",
    "\n",
    "The second change Audacity is A/B testing is a more career focused description on a course overview page. They hope that this change may encourage more users to enroll and complete this course. In this experiment, we’re going to analyze the following metrics:\n",
    "\n",
    "1. **Enrollment Rate**: Click through rate for the Enroll button the course overview page\n",
    "1. **Average Reading Duration**: Average number of seconds spent on the course overview page\n",
    "1. **Average Classroom Time**: Average number of days spent in the classroom for students enrolled in the course\n",
    "1. **Completion Rate**: Course completion rate for students enrolled in the course\n",
    "\n",
    "First, let's determine if the difference observed for each metric is statistically significant individually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metric - Average Reading Duration\n",
    "Again, let's recap the steps we took to analyze the results of this A/B test.\n",
    "\n",
    "1. We computed the **observed difference** between the metric, average reading duration, for the control and experiment group.\n",
    "1. We simulated the **sampling distribution** for the difference in means (or average reading durations).\n",
    "1. We used this sampling distribution to simulate the **distribution under the null** hypothesis, by creating a random normal distribution centered at 0 with the same spread and size.\n",
    "1. We computed the **p-value** by finding the proportion of values in the null distribution that were greater than our observed difference.\n",
    "1. We used this p-value to determine the **statistical significance** of our observed difference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing Multiple Metrics\n",
    "The more metrics you evaluate, the more likely you are to observe significant differences just by chance - similar to what you saw in previous lessons with multiple tests. Luckily, this [multiple comparisons](https://en.wikipedia.org/wiki/Multiple_comparisons_problem) problem can be handled in several ways."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drawing Conclusion\n",
    "Since the Bonferroni method is too conservative when we expect correlation among metrics, we can better approach this problem with more sophisticated methods, such as the [closed testing procedure](http://en.wikipedia.org/wiki/Closed_testing_procedure), [Boole-Bonferroni bound](http://en.wikipedia.org/wiki/Bonferroni_bound), and the [Holm-Bonferroni method](http://en.wikipedia.org/wiki/Holm%E2%80%93Bonferroni_method). These are less conservative and take this correlation into account.\n",
    "\n",
    "If you do choose to use a less conservative method, just make sure the assumptions of that method are truly met in your situation, and that you're not just trying to [cheat on a p-value](http://freakonometrics.hypotheses.org/19817). Choosing a poorly suited test just to get significant results will only lead to misguided decisions that harm your company's performance in the long run."
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
