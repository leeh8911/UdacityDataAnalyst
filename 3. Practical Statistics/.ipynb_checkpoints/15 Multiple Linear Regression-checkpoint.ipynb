{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "In this lesson, you will be extending your knowledge of simple linear regression, where you were predicting a quantitative response variable using a quantitative explanatory variable. That is, you were using an equation that looked like this:\n",
    "$$\n",
    "\\hat{y} = b_0 + b_1 x_1\n",
    "$$\n",
    " \n",
    "\n",
    "In this lesson, you will learn about multiple linear regression. In these cases, you will be using both quantitative and categorical x-variables to predict a quantitative response. That is, you will be creating equations that like this to predict your response:\n",
    "$$\n",
    "\\hat{y} = b_0 + b_1 x_1 + b_2 x_2 + b_3 x_3 + b_4 x_4\n",
    "$$\n",
    "\n",
    "Furthermore, you will learn about how to assess problems that can happen in multiple linear regression, how to address these problems, and how to assess how well your model is performing. It turns out **Rsquared** can be used, but might be misleading. And, unfortunately, the correlation coefficient is only a measure of the linear relationship between **two quantitative** variables, so it will not be very useful in the multiple linear regression case.\n",
    "\n",
    "Here is a wonderful free supplementary book: [Introduction to Statistical Learning](http://www-bcf.usc.edu/~gareth/ISL/ISLR%20First%20Printing.pdf). This is an absolutely spectacular book for getting started with machine learning, and Chapter 3 discusses many of the ideas in this lesson. The programming performed in the text is in R, but here is an additional resource, not created by the book's authors, that provides Jupyter Notebooks in Python with notes and answers to nearly all the questions from the book: [https://www.reddit.com/r/learnpython/comments/6rkh3u/introduction_to_statistical_learning_with_python/](https://www.reddit.com/r/learnpython/comments/6rkh3u/introduction_to_statistical_learning_with_python/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple Linear Regression\n",
    "If you want to fully grasp **how** the functions we will be using in Python work to fit multiple linear regression models, it is absolutely necessary to have a firm grasp of linear algebra. Though you can complete this class (and fit multiple linear regression models in Python) without knowing linear algebra, linear algebra is useful for understanding why you do or do not obtain certain results from Python, as well as troubleshooting if something goes wrong with fitting your model.\n",
    "\n",
    "You will see a few instances of strange output that you may obtain from multiple linear regression output as you work through this course. Two additional resources are listed below if you need a linear algebra refresher before you continue!\n",
    "\n",
    "* [Khan Academy's free course on Linear Algebra](https://www.khanacademy.org/math/linear-algebra) in case you need a refresher.\n",
    "\n",
    "* The linear algebra (matrix math) and NumPy refresher - navigate to your Extracurricular section in the leftmost-panel of your classroom to view."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How Does MLR Work?\n",
    "The text noted at the end of the video is provided below.\n",
    "\n",
    "## How Do We Find the \"Right\" Coefficients in Multiple Linear Regression\n",
    "In the simple linear regression section, you saw how we were interested in minimizing the squared distance between each actual data point and the predicted value from our model.\n",
    "\n",
    "But in multiple linear regression, we are actually looking at points that live in not just a two dimensional space.\n",
    "\n",
    "For a full derivation of how this works, [this](https://web.stanford.edu/~mrosenfe/soc_meth_proj3/matrix_OLS_NYU_notes.pdf) article provides a breakdown of the steps.\n",
    "\n",
    "The takeaway for us is that we can find the optimal \\betaβ estimates by calculating $\\left(X^TX\\right)^{-1}X^Ty$.\n",
    "\n",
    "In the following video, you will use statsmodels to obtain the coefficients similar to how we did it in the last concept, but you will also solve for the coefficients using the equation above to show the results are not magic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple Linear Regression Model Results\n",
    "In this video, the coefficients had positive and negative values. Therefore, we can interpret each coefficient as the **predicted increase or decrease in the response for every one unit increase in the explanatory variable, holding all other variables in the model constant.**\n",
    "\n",
    "However, in general, coefficients might be positive or negative. Therefore, each coefficient is the predicted **change** in the response for every one unit increase in the explanatory variable, holding all other variables in the model constant.\n",
    "\n",
    "This interpretation is very similar to what you saw in the last lesson with the simple addition of the phrase **\"holding all other variables constant\"** meaning only the variable attached to the coefficient changes, while all other variables stay the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dummy Variables\n",
    "The way that we add categorical variables into our multiple linear regression models is by using dummy variables. The most common way dummy variables are added is through 1, 0 encoding. In this encoding method, you create a new column for each **level** of a category (in this case A, B, or C). Then our new columns either hold a 1 or 0 depending on the presence of the level in the original column.\n",
    "\n",
    "![img](./assets/l15_8.png)\n",
    "\n",
    "When we add these dummy variables to our multiple linear regression models, we always drop one of the columns. The column you drop is called the **baseline**. The coefficients you obtain from the output of your multiple linear regression models are then an indication of how the encoded levels compare to the baseline level (the dropped level)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Math Behind Dummy Variables\n",
    "In the last video, you were introduced to the idea the way that categorical variables will be changed to **dummy variables** in order to be added to your linear models.\n",
    "\n",
    "Then, you will need to drop one of the **dummy columns** in order to make your matrices full rank.\n",
    "\n",
    "If you remember back to the closed form solution for the coefficients in regression, we have \\betaβ is estimated by $\\left(X^TX\\right)^{-1}X^Ty$.\n",
    "\n",
    "In order to take the inverse of $\\left(X^TX\\right)$, the matrix $X$ must be full rank. That is, all of the columns of $X$ must be linearly independent.\n",
    "\n",
    "If you do not drop one of the columns (from the model, not from the dataframe) when creating the dummy variables, your solution is unstable and results from Python are unreliable. You will see an example of what happens if you do not drop one of the dummy columns in the next concept.\n",
    "\n",
    "The takeaway is ... **when you create dummy variables using 0, 1 encodings, you always need to drop one of the columns from the model to make sure your matrices are full rank (and that your solutions are reliable from Python).**\n",
    "\n",
    "The reason for this is linear algebra. Specifically, in order to invert matrices, a matrix must be full rank (that is, all the columns need to be linearly independent). Therefore, you need to drop one of the dummy columns, to create linearly independent columns (and a full rank matrix)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dummy Variables Recap\n",
    "Though Python and R use 1, 0 encodings in the default functionality, there are many ways we might encode dummy variables. [SAS](https://www.sas.com/en_us/software/stat.html) is a common programming language that uses a different encoding than you have used so far. Being comfortable with the binary (1, 0) encodings we've been using is really all you need, but if you are curious about other encodings you can try out the quiz on the next page to see how this works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Model Assumptions\n",
    "## Model Assumptions And How To Address Each\n",
    "Here are the five potential problems related to Multiple Linear Regression that we mentioned in the previous video, that are addressed in [Introduction to Statistical Learning](http://www-bcf.usc.edu/~gareth/ISL/ISLR%20Seventh%20Printing.pdf):\n",
    "\n",
    "1. Non-linearity of the response-predictor relationships\n",
    "1. Correlation of error terms\n",
    "1. Non-constant Variance and Normally Distributed Errors\n",
    "1. Outliers/ High leverage points\n",
    "1. Multicollinearity\n",
    "\n",
    "This text is a summary of how to identify whether these problems exist, as well as how to address them. This is a common interview question asked by statisticians, but its practical importance is hit or miss depending on the purpose of your model. In the upcoming concepts, we will look more closely at specific points that I believe deserve more attention, but below you will see a more exhaustive introduction to each topic. Let's take a closer look at each of these items.\n",
    "\n",
    "## Linearity\n",
    "The assumption of linearity is that a linear model is the relationship that truly exists between your response and predictor variables. If this isn't true, then your predictions will not be very accurate. Additionally, the linear relationships associated with your coefficients really aren't useful either.\n",
    "\n",
    "In order to assess if a linear relationship is reasonable, a plot of the residuals $\\left(y - \\hat{y}\\right)$ by the predicted values $\\left(\\hat{y}\\right)$ is often useful. If there are curvature patterns in this plot, it suggests that a linear model might not actually fit the data, and some other relationship exists between the predictor variables and response. There are many ways to create non-linear models (even using the linear model form), and you will be introduced to a few of these later in this lesson.\n",
    "\n",
    "In the image at the bottom of this page, these are considered the **biased** models. Ideally, we want to see a random scatter of points like the top left residual plot in the image.\n",
    "\n",
    "## Correlated Errors\n",
    "Correlated errors frequently occur when our data are collected over time (like in forecasting stock prices or interest rates in the future) or data are spatially related (like predicting flood or drought regions). We can often improve our predictions by using information from the past data points (for time) or the points nearby (for space).\n",
    "\n",
    "The main problem with not accounting for correlated errors is that you can often use this correlation to your advantage to better predict future events or events spatially close to one another.\n",
    "\n",
    "One of the most common ways to identify if you have correlated errors is based on the domain from which the data were collected. If you are unsure, there is a test known as a Durbin-Watson test that is commonly used to assess whether correlation of the errors is an issue. Then ARIMA or ARMA models are commonly implemented to use this correlation to make better predictions.\n",
    "\n",
    "## Non-constant Variance and Normally Distributed Errors\n",
    "Non-constant variance is when the spread of your predicted values differs depending on which value you are trying to predict. This isn't a huge problem in terms of predicting well. However, it does lead to confidence intervals and p-values that are inaccurate. Confidence intervals for the coefficients will be too wide for areas where the actual values are closer to the predicted values, but too narrow for areas where the actual values are more spread out from the predicted values.\n",
    "\n",
    "Commonly, a log (or some other transformation of the response variable is done) in order to \"get rid\" of the non-constant variance. In order to choose the transformation, a [Box-Cox](http://www.statisticshowto.com/box-cox-transformation/) is commonly used.\n",
    "\n",
    "Non-constant variance can be assessed again using a plot of the residuals by the predicted values. In the image at the bottom of the page, non-constant variance is labeled as **heteroscedastic**. Ideally, we want an unbiased model with homoscedastic residuals (consistent across the range of values).\n",
    "\n",
    "Though the text does not discuss normality of the residuals, this is an important assumption of regression if you are interested in creating reliable confidence intervals. More on this topic is provided [here](http://www.itl.nist.gov/div898/handbook/pri/section2/pri24.htm).\n",
    "\n",
    "## Outliers/Leverage Points\n",
    "Outliers and leverage points are points that lie far away from the regular trends of your data. These points can have a large influence on your solution. In practice, these points might even be typos. If you are aggregating data from multiple sources, it is possible that some of the data values were carried over incorrectly or aggregated incorrectly.\n",
    "\n",
    "Other times outliers are accurate and true data points, not necessarily measurement or data entry errors. In these cases, 'fixing' is more subjective. Often the strategy for working with these points is dependent on the goal of your analysis. Linear models using ordinary least squares, in particular, are not very robust. That is, large outliers may greatly change our results. There are techniques to combat this - largely known as **regularization** techniques. These are beyond the scope of this class, but they are quickly discussed in the [free course on machine learning](https://classroom.udacity.com/courses/ud120).\n",
    "\n",
    "## Multi-collinearity\n",
    "Multicollinearity is when we have predictor variables that are correlated with one another. One of the main concerns of multicollinearity is that it can lead to coefficients being flipped from the direction we expect from simple linear regression.\n",
    "\n",
    "One of the most common ways to identify multicollinearity is with bivariate plots or with **variance inflation factors (or VIFs)**. This is a topic we will dive into in the next concept, so we won't spend as much time on it here.\n",
    "![img](./assets/l15_16.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multicollinearity & VIFs\n",
    "You saw in this video two different ways of identifying multicollinearity:\n",
    "\n",
    "1. We can look at the correlation of each explanatory variable with each other explanatory variable (with a plot or the correlation coefficient).\n",
    "\n",
    "2. We can look at Variance Inflation Factors (VIFs) for each variable. This calculation will be shown in more detail in the next video.\n",
    "\n",
    "We saw that when x-variables are related to one another, we can have flipped relationships in our multiple linear regression models from what we would expect when looking at the bivariate linear regression relationships.\n",
    "\n",
    "For more on VIFs and multicollinearity, [here is the referenced post from the video on VIFs](https://etav.github.io/python/vif_factor_python.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multicollinearity & VIFs\n",
    "**We would like x-variables to be related to the response, but not to be related to one another.** When our x-variables are correlated with one another, this is known as **multicollinearity**. Multicollinearity has two potential negative impacts. As you saw in the previous example,\n",
    "\n",
    "1. The expected relationships between your x-variables and the response may not hold when multicollinearity is present. That is, you may expect a positive relationship between the explanatory variables and the response (based on the bivariate relationships), but in the multiple linear regression case, it turns out the relationship is negative.\n",
    "\n",
    "2. Our hypothesis testing results may not be reliable. It turns out that having correlated explanatory variables means that our coefficient estimates are less stable. That is, standard deviations (often called standard errors) associated with your regression coefficients are quite large. Therefore, a particular variable might be useful for predicting the response, but because of the relationship it has with other x-variables, you will no longer see this association.\n",
    "\n",
    "We have also looked at two different ways of identifying multicollinearity:\n",
    "\n",
    "1. Looking at the correlation of each explanatory variable with each other explanatory variable (with a plot or the correlation coefficient).\n",
    "2. Looking at VIFs for each variable.\n",
    "\n",
    "When VIFs are greater than 10, this suggests that multicollinearity is certainly a problem in your model. Some experts even suggest that VIFs of greater than 5 can be problematic. In most cases, not just one VIF is high, but rather many VIFs are high, as these are measures of how related variables are with one another.\n",
    "\n",
    "The most common way of working with correlated explanatory variables in a multiple linear regression model is simply to remove one of the variables that is most related to the other variables. Choosing an explanatory variable that you aren't interested in, or isn't as important to you, is a common choice.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Higher Order Terms\n",
    "## How to Identify Higher Order Terms?\n",
    "Higher order terms in linear models are created when multiplying two or more x-variables by one another. Common higher order terms include **quadratics** ($x_1^2$) and cubics ($x_1^3$) , where an x-variable is multiplied by itself, as well as interactions ($x_1x_2$) , where two or more x-variables are multiplied by one another.\n",
    "![img1](./assets/l15_21_1.png)\n",
    "\n",
    "In a model with no higher order terms, you might have an equation like:\n",
    "$$\n",
    "\\hat{y} = b_0 + b_1x_1 + b_2x_2\n",
    "$$\n",
    "\n",
    "Then we might decide the linear model can be improved with higher order terms. The equation might change to:\n",
    "$$\n",
    "\\hat{y} = b_0 + b_1x_1 + b_2x_1^2 + b_3x_2 + b_4x_1x_2\n",
    "$$\n",
    "\n",
    "Here, we have introduced a quadratic ($b_2x_1^2$) and an interaction ($b_4x_1x_2$) term into the model.\n",
    "\n",
    "In general, these terms can help you fit more complex relationships in your data. However, they also take away from the ease of interpreting coefficients, as we have seen so far. You might be wondering: \"How do I identify if I need one of these higher order terms?\"\n",
    "\n",
    "When creating models with **quadratic**, **cubic**, or even higher orders of a variable, we are essentially looking at how many curves there are in the relationship between the explanatory and response variables.\n",
    "\n",
    "If there is one curve, like in the plot below, then you will want to add a quadratic. Clearly, we can see a line isn't the best fit for this relationship.\n",
    "![img2](./assets/l15_21_2.png)\n",
    "\n",
    "Then, if we want to add a cubic relationship, it is because we see two curves in the relationship between the explanatory and response variable. An example of this is shown in the plot below.\n",
    "![img3](./assets/l15_21_3.jpg)\n",
    "[https://tamino.wordpress.com/2011/03/31/so-what/](https://tamino.wordpress.com/2011/03/31/so-what/)\n",
    "\n",
    "Diving into these relationships a little more closely and creating them in your linear models in Python will be the focus in the upcoming videos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpreting Interactions\n",
    "## Interaction Terms\n",
    "In the previous video, you were introduced to how you might interpret interactions, and how you might observe the need for an interaction in your model.\n",
    "\n",
    "Mathematically, an interaction is created by multiplying two variables by one another and adding this term to our linear regression model.\n",
    "![img](./assets/l15_24.png)\n",
    "\n",
    "The example from the previous video used **area** ($x_1$) and the **neighborhood** ($x_2$) of a **home** (either $A$ or $B$) to predict the home **price** ($y$). At the top of the screen in the video, you might have noticed the equation for a linear model using these variables as:\n",
    "$$\n",
    "\\hat{y} = b_0 + b_1x_1 + b_2x_2\n",
    "$$\n",
    "\n",
    "This example does not involve an interaction term, and this model is appropriate if the relationship of the variables looks like that in the plot below.\n",
    "![img1](./assets/l15_24_1.png)\n",
    "\n",
    "where $b_1$ is the way we estimate the relationship between **area** and **price**, which in this model we believe to be the same regardless of the neighborhood.\n",
    "\n",
    "Then $b_2$ is the difference in price depending on which neighborhood you are in, which is the **vertical** distance between the two lines here:\n",
    "![img1](./assets/l15_24_2.png)\n",
    "\n",
    "Notice here that:\n",
    "\n",
    "* The way that **area** is related to **price** is the same regardless of **neighborhood**.\n",
    "\n",
    "AND\n",
    "\n",
    "* The difference in **price** for the different **neighborhoods** is the same regardless of the **area**.\n",
    "\n",
    "When these statements are true, we do not need an interaction term in our model. However, we need an interaction when **the way that area is related to price is different depending on the neighborhood**.\n",
    "\n",
    "Mathematically, when the way area relates to price depends on the neighborhood, this suggests we should add an interaction. By adding the interaction, we allow the slopes of the line for each neighborhood to be different, as shown in the plot below. Here we have added the interaction, and you can see this allows for a difference in these two slopes.\n",
    "![img1](./assets/l15_24_3.png)\n",
    "\n",
    "These lines might even cross or grow apart quickly. Either of these would suggest an interaction is present between **area** and **neighborhood** in the way they related to the **price**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recap\n",
    "1. You learned how to build a multiple linear regression model in Python, which was actually very similar to what you did in the last lesson on simple linear regression.\n",
    "\n",
    "\n",
    "2. You learned how to encode dummy variables, and interpret the coefficients attached to each.\n",
    "\n",
    "\n",
    "3. You learned about higher order terms, and how this impacts your ability to interpret coefficients.\n",
    "\n",
    "\n",
    "4. You learned how to identify what it would mean for an interaction to be needed in a multiple linear regression model, as well as how to identify other higher order terms. But again, these do make interpreting coefficients directly less of a priority, and move your model towards one that, rather, aims to predict better at the expense of interpretation.\n",
    "\n",
    "\n",
    "5. You learned about the model assumptions, and we took a closer look at multicollinearity. You learned about variance inflation factors, and how multicollinearity impacts the model coefficients and standard errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
