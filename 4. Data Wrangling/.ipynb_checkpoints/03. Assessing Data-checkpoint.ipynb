{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assessing\n",
    "**Assessing** your data is the second step in data wrangling. When assessing, you're like a detective at work, inspecting your dataset for two things: data quality issues (i.e. content issues) and lack of tidiness (i.e. structural issues).\n",
    "\n",
    "Assessing is the precursor to cleaning. You can't clean something that you don't know exists! In this lesson, you'll learn to identify and categorize common data quality and tidiness issues. This lesson is the shortest and most \"hands-off\" code-wise of all four in the course because of the passive nature of assessing relative to gathering and cleaning. We have tried to include quizzes wherever possible.\n",
    "\n",
    "## This lesson will be structured as follows:\n",
    "\n",
    "* You'll get motivated to assess (and later clean) the dataset for lessons 3 and 4: Phase II clinical trial data that compares the efficacy and safety of a new oral insulin to treat diabetes\n",
    "* You'll learn to distinguish between dirty data and messy data\n",
    "* You'll assess the data visually and programmatically to identify:\n",
    "    * Data quality issues\n",
    "    * Tidiness issues\n",
    "* You'll learn about data quality dimensions and categorize each of the data quality issues identified above into its appropriate dimension\n",
    "\n",
    "To begin, I want to introduce you to the dataset you will be assessing in this lesson."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset: Oral Insulin Phase II Clinical Trial Data\n",
    "## DISCLAIMER: This Data Isn't \"Real\"\n",
    "The Auralin and Novodra are **not** real insulin products. This clinical trial data was fabricated for the sake of this course. When assessing this data, the issues that you'll detect (and later clean) are meant to simulate real-world data quality and tidiness issues.\n",
    "\n",
    "That said:\n",
    "\n",
    "* This dataset was constructed with the consultation of real doctors to ensure plausibility.\n",
    "* This clinical trial data for an alternative insulin was inspired and closely mimics this real [clinical trial for a new inhaled insulin called Afrezza](http://care.diabetesjournals.org/content/38/12/2266.long).\n",
    "* The data quality issues in this dataset mimic real, [common data quality issues in healthcare data](http://media.hypersites.com/clients/1446/filemanager/Articles/DocCenter_Problem_with_data.pdf). These issues impact quality of care, patient registration, and revenue.\n",
    "* The patients in this dataset were created using this [fake name generator](http://www.fakenamegenerator.com/order.php) and do not include real names, addresses, phone numbers, emails, etc.\n",
    "\n",
    "The video above is only a short preview of the dataset that is intended to motivate. So don't worry if the details don't all make sense right now. You'll get intimately familiar with each column in each table in the dataset shortly. If you want to dive deeper into the data now, hop ahead to the **Visual Assessment: Acquaint Yourself** page where the data files are provided in a Jupyter Notebook workspace. (You can also download the files from there if you'd like by clicking the Jupyter logo in the workspace then selecting and downloading each file.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assessment: Types vs. Steps\n",
    "Why should we first document unclean issues we observe, rather than just write what we need to do to fix the issues? When your data's issues get complicated, writing how to fix each can get confusing, lengthy, and time-consuming. It can get overwhelming trying to think of how to clean something complicated immediately after documenting it.\n",
    "\n",
    "If you are separating the assessing and cleaning steps of data wrangling, as we are in this lesson, writing only observations as a first step is good practice.\n",
    "\n",
    "If you choose to assess an issue then immediately clean that issue (which is very much allowed), you can skip the observation and go straight to defining how to clean it (which is part of the *Define-Code-Test* cleaning framework youâ€™ll see in Lesson 4).\n",
    "\n",
    "*Note: Visualizing your data (i.e., creating plots) is part of Programmatic rather than Visual Assessment. Tricky! This is because plotting data requires coding, or programming.*\n",
    "\n",
    "## In this Lesson\n",
    "You're going to start with visual assessment in the first part of this lesson to identify data quality issues. You'll then use programmatic assessment to identify some more data quality issues.\n",
    "\n",
    "Toward the end of the lesson, you'll use visual (first) and programmatic assessment (second) to identify tidiness issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quality: Visual Assessment1\n",
    "**More Information**\n",
    "* [Stack Overflow: Is it a good idea to use an integer column for storing US ZIP codes in a database?](https://stackoverflow.com/questions/893454/is-it-a-good-idea-to-use-an-integer-column-for-storing-us-zip-codes-in-a-databas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assessing vs. Exploring\n",
    "In the context of this dataset, **assessing** is everything you just identified, like spotting:\n",
    "\n",
    "* Missing HbA1c changes\n",
    "* Poorly formatted zip codes (e.g., four digits and float data type instead of five digits and string or object data type)\n",
    "* Multiple state formats (e.g., NY and New York)\n",
    "* Incorrect patient height values (e.g., 27 inches instead of 72 inches)\n",
    "\n",
    "**Assessing** is also identifying structural (tidiness) issues that make analysis difficult.\n",
    "\n",
    "The discovery of these data quality and ensure that the analysis can be executed, which for this clinical trial data includes calculated average patient metrics (e.g. age, weight, height, and BMI) and calculating the confidence interval for the difference in HbA1c change means between Novodra and Auralin patients.\n",
    "\n",
    "**Exploring**, in the context of this dataset, might be:\n",
    "\n",
    "* Using summary statistics like `count` on the state column or `mean` on the weight column to see if patients from certain states or of certain weights are more likely to have diabetes, which we can use to exclude certain patients from the analysis and make it less biased\n",
    "\n",
    "Exploring, in the context of a clinical trial, is less likely to happen given that clinical trials are expensive and consist of extreme pre-planning. So exploring on this dataset would likely exclusively happen before the treatments and adverse_reactions tables were created, i.e., before the clinical trial was conducted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quality: Visual Assessment2\n",
    "**More Information**\n",
    "* [ABBYY: Optical Character Recognition](https://www.abbyy.com/en-ca/finereader/what-is-ocr/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Quality Dimensions1\n",
    "Data quality dimensions help guide your thought process while assessing and also cleaning. The four main data quality dimensions are:\n",
    "\n",
    "* **Completeness**: do we have all of the records that we should? Do we have missing records or not? Are there specific rows, columns, or cells missing?\n",
    "* **Validity**: we have the records, but they're not valid, i.e., they don't conform to a defined schema. A schema is a defined set of rules for data. These rules can be real-world constraints (e.g. negative height is impossible) and table-specific constraints (e.g. unique key constraints in tables).\n",
    "* **Accuracy**: inaccurate data is wrong data that is valid. It adheres to the defined schema, but it is still incorrect. Example: a patient's weight that is 5 lbs too heavy because the scale was faulty.\n",
    "* **Consistency**: inconsistent data is both valid and accurate, but there are multiple correct ways of referring to the same thing. Consistency, i.e., a standard format, in columns that represent the same data across tables and/or within tables is desired.\n",
    "\n",
    "Regarding the other data quality research mentioned in the video, the additional dimensions are super specific cases of these four dimensions listed above. Example: currency, defined as follows: the degree to which data is current with the world that it models. Currency can measure how up-to-date data is. Currency is a specific case of accuracy data in the sense that out-of-date data is (usually) valid but wrong. In other words, our definition of accuracy can include currency.\n",
    "\n",
    "More Information\n",
    "The inconsistent data quality dimension research mentioned in the video: [source 1 (PDF)](http://www.damauk.org/RWFilePub.php?&cat=403&dx=2&ob=3&rpn=catviewleafpublic403&id=106193), [source 2](http://www.informit.com/articles/article.aspx?p=399325&seqNum=3), [source 3](http://searchdatamanagement.techtarget.com/definition/data-quality), and [source 4](https://www.youtube.com/watch?v=dPsx8_Fcr-U)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quality: Programmatic Assessment1\n",
    "In the above video, I state that having the **country** column as the data type object (string) is fine, while I argue that **state** should be the category data type. This topic deserves a little bit more discussion.\n",
    "\n",
    "**state** is categorical because its values are a finite set of options without order. **country**, for all intents and purposes, also has a finite set of values and therefore could be argued to be of categorical type as well. It seems there isn't much freedom of values in **country** to deserve classifying it as a string.\n",
    "\n",
    "So why use object here for the data type for **country**? Well, **country** does still have a lot of values. Categorical data with tons of categories isn't that useful. Another reason for using object here is situational, i.e., it depends on the context in which you'd like to use the **country** column. In this dataset, all of the clinical trial patients are from the United States, so there are no advantages gained from switching the data type from object to category. The **country** column won't be used for analysis.\n",
    "\n",
    "A more general scenario outside of this dataset is as follows. Say you had one to a few observations from each **country**, it would probably be best to treat country like a string and group observations on a larger unit, like **world_region** (Africa, Asia, Central America, etc.). If you had a lot of observations from a few countries, like test scores from students sampled in a handful of countries, making **country** categorical would be more appropriate.\n",
    "\n",
    "The answer to a lot of questions in data analysis and data science is \"it depends.\" This is what makes wrangling tricky sometimes since you have to understand the context of your data to make the best decision. Data scientists in a workplace should often consult with others on the team who know the data context best, or who will use the results of analysis later, like business analysts or product owners."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How Data Gets Dirty and Messy\n",
    "## Sources of Dirty Data\n",
    "*Dirty data = low quality data = content issues*\n",
    "\n",
    "There are lots of sources of dirty data. Basically, anytime humans are involved, there's going to be dirty data. There are lots of ways in which we touch data we work with.\n",
    "\n",
    "* We're going to have user entry errors.\n",
    "* In some situations, we won't have any data coding standards, or where we do have standards they'll be poorly applied, causing problems in the resulting data\n",
    "* We might have to integrate data where different schemas have been used for the same type of item.\n",
    "* We'll have legacy data systems, where data wasn't coded when disc and memory constraints were much more restrictive than they are now. Over time systems evolve. Needs change, and data changes.\n",
    "* Some of our data won't have the unique identifiers it should.\n",
    "* Other data will be lost in transformation from one format to another.\n",
    "* And then, of course, there's always programmer error.\n",
    "* And finally, data might have been corrupted in transmission or storage by cosmic rays or other physical phenomenon. So hey, one that's not our fault.\n",
    "\n",
    "## Sources of Messy Data\n",
    "Messy data = untidy data = structural issues\n",
    "\n",
    "Messy data is usually the result of poor data planning. Or a lack of awareness of the benefits of [tidy data](https://cran.r-project.org/web/packages/tidyr/vignettes/tidy-data.html). Fortunately, messy data is usually much more easily addressable than most of the sources of dirty data mentioned above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# You Can Iterate!\n",
    "The concept of iterating isn't that applicable for clinical trials given the rigor involved in their planning. But, theoretically, the following situations could arise that require iteration:\n",
    "\n",
    "* Maybe you (as the data analyst or data scientist on the clinical trial research team) realized your statistical power calculations were wrong, and you needed to recruit more patients to make your study statistically significant. You'd also have to do revisit gathering in this scenario.\n",
    "* Maybe you realized you were missing a key piece of patient information, like patient blood type (again, unlikely given the rigor of clinical trials, but mistakes happen) because you discovered new research that related insulin resistance to blood type. You'd also have to do revisit gathering in this scenario.\n",
    "* Maybe you finished assessing, started cleaning, and spotted another data quality issue. Revisiting assessing to add these assessments to your notes is fine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assessing Summary\n",
    "Assessing is the second step in the data wrangling process:\n",
    "\n",
    "* Gather\n",
    "* **Assess**\n",
    "* Clean\n",
    "\n",
    "You can assess data for:\n",
    "\n",
    "* Quality: issues with content. Low quality data is also known as dirty data.\n",
    "* Tidiness: issues with structure that prevent easy analysis. Untidy data is also known as messy data. Tidy data requirements:\n",
    "    1. Each variable forms a column.\n",
    "    1. Each observation forms a row.\n",
    "    1. Each type of observational unit forms a table.\n",
    "    \n",
    "...using two types of assessment:\n",
    "\n",
    "* Visual assessment: scrolling through the data in your preferred software application (Google Sheets, Excel, a text editor, etc.).\n",
    "* Programmatic assessment: using code to view specific portions and summaries of the data (pandas' `head`, `tail`, and `info` methods, for example)."
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
